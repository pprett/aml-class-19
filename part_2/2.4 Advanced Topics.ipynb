{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Topics in AML\n",
    "\n",
    "## I Leakage Detection\n",
    "\n",
    "## II Automated Feature Engineering\n",
    "\n",
    "## III Transfer Learning\n",
    "\n",
    "## IV Model Selection at Scale\n",
    "\n",
    "## V Open Research Topics in AML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Leakage Detection\n",
    "\n",
    "\n",
    "<img src=\"img/guts-leakage-nutshell.png\" width=600>\n",
    "\n",
    "Training on **contaminated data** leads to overly optimistic expectations about model performance in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[ODSC 2018 talk by Yuriy Guts on Target Leakage.](https://github.com/YuriyGuts/odsc-target-leakage-workshop)\n",
    "\n",
    "<img src=\"img/guts-target-leakage.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can AML help in preventing Target Leakage?\n",
    "\n",
    "### Data Collection\n",
    "Assist Data Scientist to flag spurious columns (e.g. very high uni-variate correlation). \n",
    "\n",
    "### Feature Engineering\n",
    "Pipeline DSLs can provide safty - use them (even when doing manual ML).\n",
    "\n",
    "### Partitioning\n",
    "Assist Data Scientist by providing good error diagnostics and model introspection.\n",
    "\n",
    "### Training & Tuning\n",
    "Human-out-of-the-loop: implement best pracise, no short cuts, no peeking at the holdout data and trying yet another model.\n",
    "\n",
    "### ML Competitions\n",
    "AML needs to have a switch to turn off leakage prevention. :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automated Feature Engineering\n",
    "\n",
    "<img src=\"img/kanter-fe-process.png\" width=600>\n",
    "<div style=\"text-align: right\">Source: J. Kanter, K. Veeramachaneni (2015), Deep feature synthesis: Towards automating data science endeavors. DSAA 2015</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real-world use-cases\n",
    "Many real-world use-cases operate on relational & transactional data sources.\n",
    "\n",
    "<img src=\"img/diabetes-schema.png\">\n",
    "\n",
    "Most ML feature engineering we've seen so far operate on de-normalized data (ie a single table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Taking time into account...\n",
    "\n",
    "You need to take time into account when doing *model selection* and *feature engineering*!\n",
    "\n",
    "<img src=\"img/ge-flight-quest.jpg\">\n",
    "\n",
    "Example: predict the gate / runway arrival time for flights in mid-air ($t_0$ aka *cutoff*); you must not use records from the table `weather_station` that corresponds to sensor readings at time $t_1 > t_0$ when creating the feature `ceiling_at_destination`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ... matters\n",
    "\n",
    "<img src=\"img/ge-flight-quest-gxav.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Feature Synthesis\n",
    "\n",
    "DFS is an algorithm for automatically generating features for relational datasets. \n",
    "DFS operates on a snowflake schema and follows relationships in the data to a base field, and then sequentially\n",
    "applies mathematical functions along that path to create the final feature.\n",
    "\n",
    "<img src=\"img/kanter-dfs.png\" width=600>\n",
    "<div style=\"text-align: right\">Source: J. Kanter, K. Veeramachaneni (2015), Deep feature synthesis: Towards automating data science endeavors. DSAA 2015</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Featuretools](https://docs.featuretools.com/index.html)\n",
    "Open source implementation of DFS on top of [pandas](https://pandas.pydata.org/):\n",
    "```python\n",
    "import featuretools as ft\n",
    "data = ft.demo.load_mock_customer()\n",
    "entities = {\n",
    "   \"customers\" : (data['customers'], \"customer_id\"),\n",
    "   \"sessions\" : (data['sessions'], \"session_id\", \"session_start\"),\n",
    "   \"transactions\" : (data['transactions'], \"transaction_id\", \"transaction_time\")\n",
    "   }\n",
    "relationships = [(\"sessions\", \"session_id\", \"transactions\", \"session_id\"),\n",
    "                 (\"customers\", \"customer_id\", \"sessions\", \"customer_id\")]\n",
    "feature_matrix_customers, features_defs = ft.dfs(entities=entities,\n",
    "     relationships=relationships,\n",
    "     target_entity=\"customers\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AFE summary\n",
    "\n",
    "Generic Extract-Transform-Load (ETL) tools are not great for feature engineering\n",
    "\n",
    "  * Join operations need to take cutoffs into account.\n",
    "  * Feature extractors need to be part of your ML pipeline to guard against leakage and train-test skew.\n",
    "  \n",
    "Data needs to be stored in a way AFE can be applied\n",
    "\n",
    "  * AFE operates on a *Transaction Log* rather than a static snapshot (e.g. customer table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "<img src=\"img/pan-transfer-learning.png\" width=600>\n",
    "<div style=\"text-align: right\">Source: S. J. Pan, Q. Yang (2010), \"A Survey on Transfer Learning\" IEEE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatically Select the best Source Task\n",
    "\n",
    "Goal: Build a sentiment classifier for book reviews but little to no data for the target task is available, however, we have plenty of reviews for dvds, electronics and kitchen appliances. \n",
    "\n",
    "Which source domain (dvd, electronics, kitchen appliances) should we use to train our sentiment classifier?\n",
    "\n",
    "### Discriminative Distance (aka *Discrepancy*)\n",
    "\n",
    "Use a classifier to distinguish source and target domains. The source domain that is the hardest to separate from the target has the smallest *discriminative distance* and thus most resembles the target.\n",
    "\n",
    "<img src=\"img/blitzer-discrepancy.png\" width=600>\n",
    "<div style=\"text-align: right\">Source: J. Blitzer, H. Daume III, Domain Adaptation</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"float: left;\">\n",
    "<img src=\"img/norvig-effectiveness-of-data.png\">\n",
    "<div style=\"text-align: right\">Source: P. Norvig et al, The Unreasonable Effectiveness of Data</div>\n",
    "</div>\n",
    "<div style=\"float: right; \">\n",
    "<img src=\"img/banko-learning-curve.png\">\n",
    "<div style=\"text-align: right\">Source: M. Banko, E. Brill (2001), Scaling to Very Very Large <p/> Corpora for\n",
    "Natural Language Disambiguation, ACL</div>\n",
    "</div>\n",
    "<div style=\"clear: both; \"/>\n",
    "\n",
    "### More data trumps better algorithms?\n",
    "\n",
    "Both Norvig et al. and Banko & Brill work have often been misinterpreted to mean: *more data trumps better algorihtms*.\n",
    "\n",
    "We cannot make such a general statement, the answer lies in whether you have a bias or a variance problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Complexity and Overfitting\n",
    "\n",
    "<img src=\"img/msas-little-data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More data to the rescue?\n",
    "\n",
    "<img src=\"img/msas-more-data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfitting or Overfitting?\n",
    "\n",
    "<img src=\"img/msas-learning-curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges at Scale\n",
    "\n",
    " * Why learning with more data is harder?\n",
    "   - **Paradox**: we could use more complex models due to more data but we cannot because of computational constraints [1].\n",
    "    - => we need more efficient ways for creating complex models!\n",
    "\n",
    " * Need to account for the combined cost: model fitting + model selection / tuning\n",
    "   - Smart hyperparameter tuning tries to decrease the # of model fits\n",
    "   - we can accomplish this with fewer hyperparameters too[2]\n",
    "   \n",
    "\n",
    "[1] P. Domingos, *A few useful things to know about machine learning*, 2012\n",
    "\n",
    "[2] Practitioners often favor algorithms with few hyperparameters such as RandomForest or [AveragedPerceptron](http://nlpers.blogspot.co.at/2014/10/hyperparameter-search-bayesian.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case-study: binary classification on 1TB of data\n",
    "\n",
    " * Criteo click through data\n",
    " * Down sampled ads impression data on 24 days \n",
    " * Fully anonymized dataset:\n",
    "   - 1 target\n",
    "   - 13 integer features\n",
    "   - 26 hashed categorical features\n",
    "\n",
    " * Experiment setup:\n",
    "  - Using day 0 - day 22 data for training, day 23 data for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data?\n",
    "\n",
    "Data size:\n",
    " * ~46GB/day\n",
    " * ~180,000,000/day\n",
    "\n",
    "However it is very imbalanced (even after downsampling non-events)\n",
    " * ~3.5% events rate\n",
    "\n",
    "Further downsampling of non-events to a balanced dataset will reduce the size of data to ~70GB\n",
    "  * Will fit into a single node under “optimal” conditions\n",
    "  * Loss of model accuracy is negligible in most situations\n",
    "\n",
    "Assuming 0.1% raw event (click through) rate:\n",
    "<img src=\"img/criteo-sampling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where to start?\n",
    "\n",
    " * 70GB (~260,000,000 data points) is still a lot of data\n",
    " * Let’s take a tiny slice of that to experiment\n",
    "  - Take 0.25%, then .5%, then 1%, and do grid search on them\n",
    "\n",
    "<img src=\"img/criteo-where-to-start.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GBM is the way to go, let’s go up to 10% data\n",
    "\n",
    "<img src=\"img/criteo-gbm-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A “Fairer” Way of Comparing Models\n",
    "\n",
    "<img src=\"img/criteo-gbm-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can We Extrapolate?\n",
    "\n",
    "<img src=\"img/criteo-gbm-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree Depth vs Data Size\n",
    "\n",
    "<img src=\"img/criteo-gbm-4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Open Research Topics in AML\n",
    "\n",
    "* ____ at scale\n",
    "  * Pipeline Optimization\n",
    "  * Automated Feature Engineering\n",
    "* Automated Leakage Detection\n",
    "* Automated Partitioning/Cross-Validation\n",
    "* Automated Metric Selection\n",
    "* Automated Transfer Learning\n",
    "* Automated Data Drift Detection\n",
    "  - the world never stops changing...\n",
    "* Automated Feedback Cycle Detection\n",
    "  - ... and we never stop changing the world\n",
    "\n",
    "More see [Rich Caruna's IMCL 2015 talk on AutoML](https://indico.lal.in2p3.fr/event/2914/contributions/6481/attachments/6048/7173/CaruanaAutoMLWorkshopICML2015rev4.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
