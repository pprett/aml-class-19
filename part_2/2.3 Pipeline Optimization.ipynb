{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipeline Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Search over Pipeline-land...\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approaches\n",
    "\n",
    "### Bayesian Optimization\n",
    "\n",
    "### Genetic Programming\n",
    "\n",
    "### Meta-learning\n",
    "\n",
    "### Multi-armed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "Can we use the same approach that we use for Hyper-parameter Optimization to search for the best ML pipeline?\n",
    "\n",
    "### Problem\n",
    "Parameter space becomes huge and we need to handle *conditional parameters*, parameters that are only relevant when others are set in a specific way (e.g. `svm__kernel` only relevant if `classifier=svm`).\n",
    "This has been refered to as the CASH problem: *Combined Algorithm Selection and Hyper-parameter optimization*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Genetic Programming\n",
    "\n",
    "GP is a evolutionary computation technique for automatically constructing computer programs.\n",
    "\n",
    "GP evolves computer programs, traditionally represented in memory as tree structures. Every tree node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate (e.g. for symbolic regression).\n",
    "\n",
    "[Algorithm](http://www.genetic-programming.com/gpanimatedtutorial.html):\n",
    "\n",
    "  1. Randomly create an initial population (generation 0)\n",
    "  2. Iteratively perform the following sub-steps (called a generation):\n",
    "    1. Execute each program in the population and ascertain its fitness\n",
    "    2. Select programs from the population with a proba based on fitness to participate in the genetic operations in C.\n",
    "    3. Create new programs for the population by applying the following genetic operations with specified probabilities:\n",
    "      1. Reproduction: Copy the selected individual program to the new population.\n",
    "      2. Crossover: Create new offspring program(s) for the new population by recombining randomly chosen parts from two selected programs.\n",
    "      3. Mutation: Create one new offspring program for the new population by randomly mutating a randomly chosen part of one selected program.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutation\n",
    "\n",
    "<img src=\"img/gp-mutation.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-over\n",
    "\n",
    "<img src=\"img/gp-cross-over.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meta-Learning\n",
    "\n",
    "Learn from the performance of ML piplines from **different** datasets to predict which ML pipelines will likely do well on the dataset at hand.\n",
    "\n",
    "Can we used to create a seed set for other techniques (e.g. [auto-sklearn](https://www.automl.org/automl/auto-sklearn/) uses Meta-Learning and Bayesian Optimization). \n",
    "\n",
    "\n",
    "Meta-learning model: target is performance and runtime; features are characteristics of the dataset, pipeline, and problem formulation.\n",
    "\n",
    "Training data: [OpenML](https://www.openml.org/) . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-armed Bandits\n",
    "\n",
    "### Idea\n",
    "Sample randomly N pipelines, each of them represents an *arm*. We want to select the arm with the highest yield (ie best performance) by spending the least *budget* in total. \n",
    "\n",
    "Budget can be iterations (for anytime algorithms), data points (!), or features (!).\n",
    "\n",
    "Can be seen as an application of early-stopping to pipeline optimization.\n",
    "\n",
    "<img src=\"img/map-sh.gif\" width=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case-study: [TPOT](http://epistasislab.github.io/tpot/)\n",
    "\n",
    "AML tool that optimizes machine learning pipelines using *genetic programming*.\n",
    "\n",
    "<img src=\"img/tpot-logo.jpg\" width=\"200\" style=\"float: right;\">\n",
    "\n",
    "Developed by Randal S. Olson and others at the University of Pennsylvania.\n",
    "\n",
    "<img src=\"img/tpot-ml-pipeline.png\"  width=\"600\" style=\"float: left;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/aml/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=300, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -15.341410475898769\n",
      "Generation 2 - Current best internal CV score: -15.013889792264782\n",
      "Generation 3 - Current best internal CV score: -14.706598258778271\n",
      "Generation 4 - Current best internal CV score: -14.706598258778271\n",
      "Generation 5 - Current best internal CV score: -13.721238360793881\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(input_matrix, bootstrap=True, max_features=0.6500000000000001, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
      "-7.4721440763637945\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load tpot_boston_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was:-13.721238360793881\n",
    "exported_pipeline = ExtraTreesRegressor(bootstrap=True, max_features=0.6500000000000001, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
