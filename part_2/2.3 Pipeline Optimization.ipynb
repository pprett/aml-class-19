{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipeline Optimization\n",
    "\n",
    "<img src=\"img/pipeline.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Search over Pipeline-land...\n",
    "\n",
    "<img src=\"img/caruna15-ml-pipeline-research.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What do we want?\n",
    "\n",
    "* Strong Anytime Performance\n",
    "* Strong Final Performance\n",
    "* Effective Use of Parallel Resources\n",
    "* Scalability\n",
    "* Robustness & Flexibility\n",
    "* Simplicity\n",
    "* Computational efficiency\n",
    "\n",
    "See [A. Biedenkapp, *BOHB Robust and Efficient Hyperparameter Optimization at Scale*](https://www.automl.org/blog_bohb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approaches\n",
    "\n",
    "### Bayesian Optimization\n",
    "\n",
    "### Genetic Programming\n",
    "\n",
    "### Meta-learning\n",
    "\n",
    "### Multi-armed bandits\n",
    "\n",
    "### Reinforcement Learning for Neural Architecture Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "Can we use the same approach that we use for Hyper-parameter Optimization to search for the best ML pipeline?\n",
    "\n",
    "### Problem\n",
    "Parameter space becomes huge and we need to handle *conditional parameters*, parameters that are only relevant when others are set in a specific way (e.g. `svm__kernel` only relevant if `classifier=svm`).\n",
    "This has been refered to as the CASH problem: *Combined Algorithm Selection and Hyper-parameter optimization*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Genetic Programming\n",
    "\n",
    "GP is a evolutionary computation technique for automatically constructing computer programs.\n",
    "\n",
    "GP evolves computer programs, traditionally represented in memory as tree structures. Every tree node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate (e.g. for symbolic regression).\n",
    "\n",
    "[Algorithm](http://www.genetic-programming.com/gpanimatedtutorial.html):\n",
    "\n",
    "  1. Randomly create an initial population (generation 0)\n",
    "  2. Iteratively perform the following sub-steps (called a generation):\n",
    "    1. Execute each program in the population and ascertain its fitness\n",
    "    2. Select programs from the population with a proba based on fitness to participate in the genetic operations in C.\n",
    "    3. Create new programs for the population by applying the following genetic operations with specified probabilities:\n",
    "      1. Reproduction: Copy the selected individual program to the new population.\n",
    "      2. Crossover: Create new offspring program(s) for the new population by recombining randomly chosen parts from two selected programs.\n",
    "      3. Mutation: Create one new offspring program for the new population by randomly mutating a randomly chosen part of one selected program.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutation\n",
    "\n",
    "<img src=\"img/gp-mutation.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-over\n",
    "\n",
    "<img src=\"img/gp-cross-over.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meta-Learning\n",
    "\n",
    "Learn from the performance of ML piplines from **different** datasets to predict which ML pipelines will likely do well on the dataset at hand.\n",
    "\n",
    "Can we used to create a seed set for other techniques (e.g. [auto-sklearn](https://www.automl.org/automl/auto-sklearn/) uses Meta-Learning and Bayesian Optimization). \n",
    "\n",
    "Meta-learning model: target is performance and runtime; features are characteristics of the dataset, pipeline, and problem formulation.\n",
    "\n",
    "Training data: [OpenML](https://www.openml.org/) .\n",
    "\n",
    "Portfolio selection for diverse initial solutions .\n",
    "\n",
    "<div  style=\"float: right;\" >\n",
    "<img src=\"img/automl-portfolio-selection.gif\" width=300>\n",
    "<div style=\"text-align: right\">Source: A. Biedenkapp, \"BOHB Robust and Efficient Hyperparameter Optimization at Scale.\"</div>\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-armed Bandits\n",
    "\n",
    "### Idea\n",
    "Sample randomly N pipelines, each of them represents an *arm*. We want to select the arm with the highest yield (ie best performance) by spending the least *budget* in total. \n",
    "\n",
    "### [SuccessiveHalving](http://proceedings.mlr.press/v51/jamieson16.pdf)\n",
    "HS (or its extension *Hyperband*) is a bandit technique.\n",
    "Budget can be iterations (for anytime algorithms), data points (!), or features (!).\n",
    "\n",
    "<img src=\"img/map-sh.gif\" width=500 >\n",
    "<div style=\"text-align: right\">Source: A. Biedenkapp, \"BOHB Robust and Efficient Hyperparameter Optimization at Scale.\"</div>\n",
    "\n",
    "Can be seen as an application of early-stopping to pipeline optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning for Neural Architecture Search\n",
    "\n",
    "The architecture of a Neural Network can also be seen as a DAG.\n",
    "\n",
    "All of the above methods have been proposed for NAS but recent research showed excellent results with Reinforcement Learning\n",
    "\n",
    "<img src=\"img/zoph-nas.png\" width=500 >\n",
    "<div style=\"text-align: right\">Source: B. Zoph, Q. Le (2017), \"Neural Architecture Search with Reinforcement Learning\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning for NAS cont'\n",
    "\n",
    "### Controller Network\n",
    "\n",
    "Recurrent neural network that can generate neural architectures by emitting a string/token-sequence that encodes the architecture (layers, cells, hyper-parameters). The image below describes how the tokens for n-th layer are generated:\n",
    "\n",
    "<img src=\"img/zoph-rnn.png\" width=500 >\n",
    "<div style=\"text-align: right\">Source: B. Zoph, Q. Le (2017), \"Neural Architecture Search with Reinforcement Learning\"</div>\n",
    "\n",
    "Once all actions are performed (ie. tokens emitted), the network can be trained and we receive a reward, the performance metric on the validation data and the controller is updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NAS results\n",
    "\n",
    "Zoph & Le's RL approach obtained competitive performance on the CIFAR-10 and Penn Treebank benchmarks but used vast computational resources to do so (800 GPUs for 3-4 weeks)\n",
    "\n",
    "<img src=\"img/zoph-cifar10-results.png\" width=500 >\n",
    "<div style=\"text-align: right\">Source: B. Zoph, Q. Le (2017), \"Neural Architecture Search with Reinforcement Learning\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case-study: [TPOT](http://epistasislab.github.io/tpot/)\n",
    "\n",
    "AML tool that optimizes machine learning pipelines using *genetic programming*.\n",
    "\n",
    "<img src=\"img/tpot-logo.jpg\" width=\"200\" style=\"float: right;\">\n",
    "\n",
    "Developed by Randal S. Olson and others at the University of Pennsylvania.\n",
    "\n",
    "<img src=\"img/tpot-ml-pipeline.png\"  width=\"600\" style=\"float: left;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=300, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -11.12952782060607\n",
      "Generation 2 - Current best internal CV score: -11.12952782060607\n",
      "Generation 3 - Current best internal CV score: -10.707116897088166\n",
      "Generation 4 - Current best internal CV score: -10.707116897088166\n",
      "Generation 5 - Current best internal CV score: -10.707116897088166\n",
      "\n",
      "Best pipeline: LassoLarsCV(ExtraTreesRegressor(input_matrix, bootstrap=False, max_features=0.8, min_samples_leaf=2, min_samples_split=10, n_estimators=100), normalize=True)\n",
      "-10.90743162944235\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load tpot_boston_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was:-10.707116897088166\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False, max_features=0.8, min_samples_leaf=2, min_samples_split=10, n_estimators=100)),\n",
    "    LassoLarsCV(normalize=True)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State-of-the-art\n",
    "\n",
    "Many state-of-the-art approaches to Pipeline Optimization are hybrids of the above methods:\n",
    "\n",
    "  * Meta-learning for initialization and portfolio selection\n",
    "  * Bayesian Optimization for fine tuning once good candidates were identified\n",
    "  \n",
    "<img src=\"img/automl-state-of-the-art.png\" width=500 >\n",
    "<div style=\"text-align: right\">Source: A. Biedenkapp, \"BOHB Robust and Efficient Hyperparameter Optimization at Scale.\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State-of-the-art cont'\n",
    "\n",
    "### Results\n",
    "\n",
    "<img src=\"img/automl-results.png\" width=500 >\n",
    "<div style=\"text-align: right\">Source: A. Biedenkapp, \"BOHB Robust and Efficient Hyperparameter Optimization at Scale.\"</div>\n",
    "\n",
    "### Academic Competitions\n",
    "\n",
    "  * [PAKDD 2019 Data Competition](https://www.4paradigm.com/competition/pakdd2019)\n",
    "  * [PAKDD 2018 Data Competition](https://www.4paradigm.com/competition/pakdd2018)\n",
    "  * [IJCNN 2015-2016 AutoML challenge](https://competitions.codalab.org/competitions/2321)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
