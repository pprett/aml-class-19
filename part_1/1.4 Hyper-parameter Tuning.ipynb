{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definition\n",
    "\n",
    "Hyper-parameters control the fitting behavior and are not learned from data.\n",
    "\n",
    "```\n",
    "estimator.get_params()\n",
    "```\n",
    "\n",
    "If you think of your estimator as a black-box, hyper-parameters are knobs on the outside of the box.\n",
    "The goal of *hyper-parameter tuning* is to set the nobs to get optimal performance.\n",
    "\n",
    "<img src=\"img/hp-tuning-two-knobs.jpg\">\n",
    "<div style=\"text-align: right\">Source: Wikipedia</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why hyper-parameters?\n",
    "\n",
    "HPs control the fitting behavior thus they \"guide\" the model search. You can think of this guidance as injecting *bias* into the model. \n",
    "\n",
    "<img src=\"img/eslii-mdl-search.png\" style=\"width:400px;\">\n",
    "<div style=\"text-align: right\">Source: T. Hastie et al. (2017) \"Elements of Statistical Learning (Ed. 2)\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples\n",
    "\n",
    "`sklearn.svm.SVC`\n",
    "  * C ... complexity, higher C means more variance can be captured.\n",
    "  * gamma ... width of the RBF kernerl, higher means more smoothness bias.\n",
    "  \n",
    "`sklearn.ensemble.RandomForestClassifier`\n",
    "  * max_depth ... the deeper the trees the more variance we can capture.\n",
    "  * n_features ... the more de-correlated trees, the more variance reduction (but the more trees needed).\n",
    "  \n",
    "`sklearn.linear_model.Ridge`\n",
    "  * alpha ... penalty on the L2 norm of the model coefficients, higher alpha more bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyper-parameter Tuning\n",
    "\n",
    "*Grid Search* \n",
    "\n",
    "    Defacto standard method for tuning hyper-parameters in the past decades.\n",
    "    \n",
    "    \n",
    "*Random Search*\n",
    "\n",
    "    Explore the hyper-parameter space randomly by drawing samples. \n",
    "    Good for high-dimensional spaces (e.g. DNN).\n",
    "    \n",
    "    \n",
    "<img src=\"img/bergstra12-grid-vs-rand.png\" style=\"width:600px;\">\n",
    "<div style=\"text-align: right\">Source: J. Bergstra and Y. Bengio (2012) \"Random Search for Hyper-Parameter Optimization\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When is Grid Search not a good fit?\n",
    "<img src=\"img/hp-tuning-many-knobs.jpg\" >\n",
    "<div style=\"text-align: right\">Source: Wikipedia</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyper-parameter Tuning in Scikit-learn\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "  * an estimator (regressor or classifier such as `sklearn.svm.SVC()`);\n",
    "  * a parameter space (e.g. `{'gamma': [0.01, 0.1, 1.0]}`);\n",
    "  * a method for searching or sampling candidates;\n",
    "  * a cross-validation scheme; and\n",
    "  * a score function (e.g. `sklearn.metrics.accuracy_score`).\n",
    "  \n",
    "## Classes\n",
    "\n",
    "  * `GridSearchCV`\n",
    "  * `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TODO: insert simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Shortcuts\n",
    "\n",
    "### Fit-once-evaluate-many\n",
    "\n",
    "Some models allow us to evaluate many hyper-parameter settings in a single fit. \n",
    "Examples: n_estimators in `RandomForest` and `GradientBoosting`; \"regularization path\" in linear models.\n",
    "    \n",
    "### Warm-starts\n",
    "\n",
    "Some models converge faster when warm started from a previous solution (with different HP settings). See [warm_start](https://scikit-learn.org/stable/glossary.html#term-warm-start) in sklearn.\n",
    "    \n",
    "### Heuristics\n",
    "\n",
    "For some hyper-parameters, good values or ranges can be compute via heuristics.\n",
    "Example: `gamma='auto'` in RBF kernel. \n",
    "    \n",
    "### Sub-sampling\n",
    "\n",
    "For some hyper-parameters, we can probe for good values on a subset of the data. Be cautious though!\n",
    "Example: `learning_rate` in SGD."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
