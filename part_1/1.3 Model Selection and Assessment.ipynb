{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection and Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definition\n",
    "\n",
    "*Model Selection*\n",
    "\n",
    "    Estimating the performance of different models in order to choose the best one.\n",
    "    \n",
    "*Model Assessment*\n",
    "\n",
    "    Estimating the generalization error of a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance Metrics\n",
    "\n",
    "Summarizes the performance of a model in a real valued score (higher is better) or error (lower is better).\n",
    "\n",
    "Most algorithms try to minimize a loss function; loss and error dont have to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to choose a Performance Metric?\n",
    "\n",
    "*Business goal*\n",
    "  \n",
    "      The performance metric should be a good proxy for the business goal. \n",
    "      Do you need to make decisions, rank items, provide accurate probabilities?\n",
    "      Eg: Ad placement and well calibrated click probabilities -> log-loss .\n",
    "      \n",
    "*Response distribution*\n",
    "  \n",
    "      Do you expect outliers in the responses? How do you want them to be handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Out-of-sample error\n",
    "\n",
    "*Out-of-sample data*\n",
    "\n",
    "    Data that you've not used to fit your model parameters.\n",
    "    \n",
    "    \n",
    "*Train-Validation-Holdout*\n",
    "\n",
    "    Its common practise to partition your data in 3 disjoin sets:\n",
    "    \n",
    "  * Train: Used to fit the model parameters.\n",
    "  * Validation: Used for model selection.\n",
    "  * Holdout: Used to estimate the generalization error.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partitioning\n",
    "\n",
    "*Random (stratified) partitioning*\n",
    "\n",
    "    Split sets randomly. Pitfall: make sure you shuffle!\n",
    "\n",
    "*Grouped partitioning*\n",
    "\n",
    "    Split sets by mutually exclusive group membership (e.g. user IDs for recommendations)\n",
    "\n",
    "*Out-of-Time partitioning*\n",
    "\n",
    "    Split sets by chronologic order. Note: extrapolation vs. interpolation.\n",
    "    Mind the gap: use gaps to forecast multiple steps ahead.\n",
    "    When estimating the generalization error: fit model on train+validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sample Efficiency\n",
    "\n",
    "If your dataset is small or your class labels are highly skewed:\n",
    "\n",
    "*k-fold cross-validation*\n",
    "\n",
    "    Split your data into k disjoint sets, train on k-1 sets, use one for validation, iterate and average error.\n",
    "    \n",
    "*time-series cross-validation*\n",
    "\n",
    "<img src=\"img/hyndman-tscv.png\" width=\"600\">\n",
    "<div style=\"text-align: right; padding-right: 220px\">Source: R. Hyndman https://robjhyndman.com/hyndsight/tscv/</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Nested cross-validation*\n",
    "\n",
    "    Multiple layers of cross-validation for model tuning, selection and assessment.\n",
    "    \n",
    "Links:\n",
    "\n",
    "  * [scikit-learn user guide](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance)\n",
    "  * [R. Hyndman TSCV](https://robjhyndman.com/hyndsight/tscv/)\n",
    "  * [Towards Data Science Article](https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# No-free-lunch Theorem\n",
    "\n",
    "Why do we need to do model selection? Aren't Deep Neural Networks the only learning algorithm we need?\n",
    "\n",
    "NFL: *whenever a learning algorithm performs well on some function, as measured by out-of-training-set generalization, it must perform poorly on some other(s).*\n",
    "\n",
    "<img src=\"img/sklearn-model-comp.png\">\n",
    "<div style=\"text-align: right\">Source: Scikit-learn User Guide</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NFL revisited\n",
    "\n",
    "There is no silver bullet...\n",
    "<img src=\"img/rolson17-ranking.png\" width=500>\n",
    "<div style=\"text-align: right\">Source: R. Olson et. al. (2017) \"Data-driven Advice for Applying Machine Learning to Bioinformatics Problems.\"</div>\n",
    "... but Gradient Boosted Trees are pretty good."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
